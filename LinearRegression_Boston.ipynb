{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n",
      "0.82313424229\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print(boston['DESCR'])\n",
    "X, y = boston.data, boston.target\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_test, y_test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logr = LogisticRegression()\n",
    "#logr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    # decision surface\n",
    "    x1_min, x1_max = X[:,0].min() - 1, X[:,0].max()+1\n",
    "    x2_min, x2_max = X[:,1].min() - 1, X[:,1].max()+1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1,xx2,Z,alpha=0.4,cmap=cmap)\n",
    "    plt.xlim(xx1.min(),xx1.max())\n",
    "    plt.xlim(xx2.min(),xx2.max())\n",
    "    # plot samples\n",
    "    X_test, y_test = X[test_idx,:], y[test_idx]\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y==cl,0],y=X[y==cl,1],alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl)\n",
    "    # test samples highlight\n",
    "    if test_idx:\n",
    "        X_test, y_test = X[test_idx,:], y[test_idx]\n",
    "        plt.scatter(X_test[:,0], X_test[:,1],c='',alpha=1.0,linewidth=1,marker='o',s=55,label='test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "[ True  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True  True  True  True  True  True  True]\n",
      "Accuracy: 0.96\n",
      "Number of misclassified samples: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# set initial parameters\n",
    "test_size = 0.9\n",
    "seed = 0\n",
    "N = 150\n",
    "LB = math.ceil(N * (1 - test_size))\n",
    "print(LB)\n",
    "UB = N\n",
    "# Logistic regression parameters\n",
    "c = 1\n",
    "# Perceptron parameters\n",
    "alpha = 0.0001\n",
    "eta0 = 0.01\n",
    "n_iter = 100\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,[2,3]]\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 0)\n",
    "# data transformation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "X_combined_std = np.vstack((X_train_std,X_test_std))\n",
    "y_combined=np.hstack((y_train,y_test))\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "rbf_feature = RBFSampler(gamma=1, random_state=seed)\n",
    "X_train_feat = rbf_feature.fit_transform(X_train)\n",
    "X_test_feat = rbf_feature.fit_transform(X_test)\n",
    "X_combined_feat = np.vstack((X_train_feat, X_test_feat))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=10 ** c, random_state=0)\n",
    "from sklearn.linear_model import Perceptron\n",
    "ppn = Perceptron(n_iter=n_iter, eta0=eta0, random_state=seed)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier()\n",
    "def t(x):\n",
    "    return {'a': X_train_std, 'b': X_train_std, 'c': X_features}.get(x, X_train_std)\n",
    "def c(x):\n",
    "    return {'a': X_combined_std, 'b': X_combined_std, 'c': X_combined_feat}.get(x, X_combined_std)\n",
    "def f(x):\n",
    "    return {'a': lr, 'b': ppn, 'c': clf}.get(x, lr)\n",
    "\n",
    "method = 'a'\n",
    "f(method).fit(t(method), y_train)\n",
    "plot_decision_regions(c(method), y_combined, classifier=f(method), test_idx=range(LB, UB))\n",
    "plt.xlabel('petal length')\n",
    "plt.ylabel('petal width')\n",
    "plt.show()\n",
    "# prediction error\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = f(method).predict(X_test_std)\n",
    "print(y_test == y_pred)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "print('Number of misclassified samples: %d' % (y_test != y_pred).sum());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "    $$\\phi(z)=\\left(1+e^{-z}\\right)^{-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\phi(z)=\\left(1+e^{-z}\\right)^{-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
